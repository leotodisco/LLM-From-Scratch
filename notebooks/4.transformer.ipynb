{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6337623",
   "metadata": {},
   "source": [
    "## Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e873bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89d142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import BasicTokenizer\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/my_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1e1f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(tokenizer: BasicTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98c64b",
   "metadata": {},
   "source": [
    "## Decoder:\n",
    "Components:\n",
    "- Token embedding layer: represents a token with a vector\n",
    "- Positional Encoding: preserves the token order \n",
    "- Self-Attention: understand and track relationships between tokens\n",
    "- Residual Connections: helps gradient flow easily through network\n",
    "- Layer Normalization: normalize the input\n",
    "- MLP layers: increase model capacity to learn\n",
    "- Block: groups the other component\n",
    "- Final Layers: to get the predictions (soft-max ad esempio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29d479",
   "metadata": {},
   "source": [
    "## Parameters:\n",
    "- Block Size: maximum sequence length\n",
    "- Embedding Size\n",
    "- Number of heads & head size\n",
    "- Number of blocks (layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d97e4d",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "## Step 1: Word e position embedding\n",
    "In this step, we convert the text into a list of tokens. Each token has an ID from the vocabulary.\n",
    "The shape of the tensor is 1x6 because we have one sentence with 6 tokens.\n",
    "\n",
    "Next, we use these tokens to find the corresponding embedding vector for each token. The vocabulary size is 1024, so each token uses its ID to look up the right vector in the token embedding table. We do the same for positional embeddings, which have 256 rows because the block size is 256. This means the model can only handle sequences with up to 256 tokens.\n",
    "\n",
    "After getting the token and positional embeddings, we add them together. This results in a tensor of size 1x6x768, where 1 is the number of inputs, 6 is the number of tokens, and 768 is the size of the embedding vectors. This output is then sent to the block layer.\n",
    "\n",
    "## Step 2: Multi Head Attention\n",
    "We take the tensor from the previous step and pass it to the multi-head attention layer. This layer has two settings: head size and number of heads. These settings split the attention block into smaller parts called heads. All heads process the input at the same time to speed up calculations.\n",
    "\n",
    "The goal of multi-head attention is to help the model focus on different parts of the input at once. Each head can learn to look at different relationships between words or tokens. Since they work in parallel, the model can understand patterns in the data more effectively.\n",
    "\n",
    "Each head produces a tensor of size 1x6x128, where 6 is the number of heads and 128 is the size of each head. We then combine all the outputs into a 1x6x768 tensor. Finally, this is passed through a feed-forward layer, which adjusts the last dimension to 768, matching the embedding size.\n",
    "\n",
    "We can stack multiple multi-head attention blocks to deepen the model's understanding of the input. This allows it to learn more complex patterns and relationships. In the image, we have stacked four layers to enhance its ability to process the data.\n",
    "\n",
    "## Step 3: Prediction Layer\n",
    "After the final attention block, the output is passed to the prediction layer. This layer contains a dense layer that reshapes the output to 1x6x1024, matching the vocabulary size. In the end, we get a probability distribution, allowing us to sample the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7a0b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "block_size = 256\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "vocab_size = get_vocab_size(tokenizer=tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2bdb1",
   "metadata": {},
   "source": [
    "## 1. Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ef200",
   "metadata": {},
   "source": [
    "The head class represents one attention head for the self-attention mechanism.\n",
    "It has three tensors: key, query, value.\n",
    "1. Query: represents what each word or token is looking for\n",
    "2. Key: meaning of each token\n",
    "3. value: info of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c954c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self attention \"\"\"\n",
    "    def __init__(self, head_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)   \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        _, T, _ = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x)  # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        weights = weights.masked_fill(\n",
    "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        weights = F.softmax(weights, dim=-1)  # (B, T, T)\n",
    "        weights = self.dropout(weights)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        out = weights @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e995e",
   "metadata": {},
   "source": [
    "## 2. MultiHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fedadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29343c",
   "metadata": {},
   "source": [
    "## 3. Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbdf686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_head: int) -> None:\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feed_forward = FeedFoward(n_embd)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.self_attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e294ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.final_linear_layer = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_tokens: Tensor of token indices of shape (batch_size, sequence_length)\n",
    "            targets: Optional tensor of target token indices of same shape as input_tokens\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (logits, loss) where logits has shape (batch_size, sequence_length, vocab_size)\n",
    "            and loss is optional cross-entropy loss if targets are provided\n",
    "        \"\"\"\n",
    "\n",
    "        B, T = input_tokens.shape\n",
    "\n",
    "        # input_tokens and targets are both (B,T) tensor of integers\n",
    "        token_embedding = self.token_embedding_table(input_tokens)  # (B,T,C)\n",
    "        positional_embedding = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = token_embedding + positional_embedding  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.final_layer_norm(x)  # (B,T,C)\n",
    "        logits = self.final_linear_layer(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input_tokens: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "                Generate new tokens given a context.\n",
    "\n",
    "                Args:>ns: Starting token indices of shape (batch_size, sequence_length)\n",
    "                        max_new_tokens: Number of new tokens to generate\n",
    "\n",
    "                Returns:\n",
    "                        Tensor of token indices of shape (batch_size, sequence_length + max_new_tokens)\n",
    "                \"\"\"\n",
    "\n",
    "        # input_tokens is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop input_tokens to the last block_size tokens\n",
    "            cropped_input = input_tokens[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self(cropped_input)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            input_tokens = torch.cat(\n",
    "                (input_tokens, idx_next), dim=1)  # (B, T+1)\n",
    "        return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89dce883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.532552 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "model = model.to(device) \n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a414489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 1032]) None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_length = 6\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "x = x.to(device)\n",
    "\n",
    "logits, loss = model(x)\n",
    "print(logits.shape, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
